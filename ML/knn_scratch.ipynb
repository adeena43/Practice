{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf3e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "isnull",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Adina\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_bunch.py:57\u001b[0m, in \u001b[0;36mBunch.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Adina\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_bunch.py:42\u001b[0m, in \u001b[0;36mBunch.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     38\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecated_key_to_warnings[key],\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m     )\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'isnull'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mduplicated()\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n",
      "File \u001b[1;32mc:\\Users\\Adina\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_bunch.py:59\u001b[0m, in \u001b[0;36mBunch.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(key)\n",
      "\u001b[1;31mAttributeError\u001b[0m: isnull"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2)**2))\n",
    "\n",
    "    def predict_single(self, x_sample):\n",
    "        distances = []\n",
    "        index = 0\n",
    "\n",
    "        for x_train_sample in self.x_train:\n",
    "            distances.append((index, self.euclidean_distance(x_sample, x_train_sample)))\n",
    "            index = index+1\n",
    "\n",
    "        distances.sort(key = lambda x:x[1])\n",
    "        k_idx = [idx for idx, _ in distances[:self.k]]\n",
    "        k_labels = [self.y_train[k_idxs] for k_idxs in k_idx]\n",
    "        most_common = Counter(k_labels).most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        return np.array([self.predict_single(x) for x in x_test])\n",
    "    \n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "# Main working from here\n",
    "\n",
    "data = load_iris()\n",
    "x= data.data\n",
    "y = data.target\n",
    "data = pd.DataFrame(x, columns=data.feature_names)\n",
    "data['target'] = y\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "print(data.isnull().sum())\n",
    "print(data.duplicated().sum())\n",
    "print(data)\n",
    "\n",
    "print(data.select_dtypes(include = [\"object\"]).columns.tolist())\n",
    "# handles missing values\n",
    "num_imputer = SimpleImputer(strategy = 'mean')\n",
    "cat_imputer = SimpleImputer(strategy = 'most_frequent')\n",
    "\n",
    "num_columns = data.select_dtypes(include = ['int64', 'float64']).columns\n",
    "cat_columns = data.select_dtypes(include = ['object']).columns\n",
    "\n",
    "data[num_columns] = num_imputer.fit_transform(data[num_columns])\n",
    "data[cat_columns] = cat_imputer.fit_transform(data[cat_columns])\n",
    "\n",
    "#handle duplicates: drop them simply\n",
    "data = data.drop_duplicates()\n",
    "# OR\n",
    "data = data.drop_duplicates(subset = ['column_name1','column_name2'])\n",
    "\n",
    "#encoding the categorical features:\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "#The other methods to encode my categorical columns are:\n",
    "#1) Label Encoder: order matters\n",
    "#2) One hot encoder: order does not matter\n",
    "#3) mapping: you want to assign each class a priority by yourself\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['column_name'] = le.fit_transform(data['column_name'])\n",
    "\n",
    "ohe1 = OneHotEncoder(drop = 'first')\n",
    "ohe2 = OneHotEncoder(dop = 'if_binary')\n",
    "data['column_name2'] = ohe1.fit_transform(data['column_name2']) # this has the same concept as dummy encoding because dropping the first column for each class\n",
    "data['column_name2'] = ohe2.fit_transform(data['column_name2']) #drops the first col always if the class in binary\n",
    "\n",
    "mapping = {\n",
    "    'High School':1,\n",
    "    'bachelors': 2,\n",
    "    'masters': 3\n",
    "}\n",
    "data['column_name3'] = data['column_name3'].map(mapping)\n",
    "\n",
    "#calculating the pearson correlation and selecting features with threshold 0.5\n",
    "corr_matrix = data.corr(method = 'pearson')\n",
    "print(corr_matrix)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.heatmap(corr_matrix, annot = True, cmap = 'coolwarm')\n",
    "plt.title(\"Correlation matrix\")\n",
    "plt.show()\n",
    "correlations = data.drop(data['target'], axis = 1).corrwith(data['target'])\n",
    "selected_features = correlations[abs(correlations) > 0.5].index\n",
    "\n",
    "#scale only the selected features from correlation calculation\n",
    "#scaling the data:\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(data[selected_features])\n",
    "#OR I can even use MinMaxScaler\n",
    "scaler2 = MinMaxScaler()\n",
    "x_scaled_again = scaler.fit_transform(data[selected_features])\n",
    "\n",
    "\n",
    "#train-test split:\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(x_scaled, y, test_size = 0.3)\n",
    "\n",
    "#train-validation split:\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size = 0.2, random_state = 42)\n",
    "model = KNN(k = 3)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "#Calculating different accuracies\n",
    "# train_acc  =accuracy_score(y_train, model.predict(x_train))\n",
    "# test_acc = accuracy_score(y_test, model.predict(x_test))\n",
    "# val_acc = accuracy_score(y_val, model.predict(x_val))\n",
    "#the first paameter is always the actual y, the second one is model's prediction\n",
    "\n",
    "#confusion_matrix:\n",
    "cm_train = confusion_matrix(y_train, model.predict(x_train))\n",
    "cm_test = confusion_matrix(y_test, model.predict(x_test))\n",
    "cm_val = confusion_matrix(y_val, model.predict(x_val))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
